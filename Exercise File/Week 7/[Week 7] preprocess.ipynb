{"cells":[{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J_o_h4L1g7LW","executionInfo":{"status":"ok","timestamp":1683501611069,"user_tz":-540,"elapsed":6561,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}},"outputId":"f2d021ba-aa24-4995-9a56-f1b0c71e7d90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.2)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n","Collecting JPype1>=0.7.0\n","  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PfpdGXwGcBqE"},"outputs":[],"source":["import os\n","import re\n","import json\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","from konlpy.tag import Okt\n","\n","\n","FILTERS = \"([~.,!?\\\"':;)(])\"\n","PAD = \"<PAD>\"\n","STD = \"<SOS>\"\n","END = \"<END>\"\n","UNK = \"<UNK>\"\n","\n","PAD_INDEX = 0\n","STD_INDEX = 1\n","END_INDEX = 2\n","UNK_INDEX = 3\n","\n","MARKER = [PAD, STD, END, UNK]\n","CHANGE_FILTER = re.compile(FILTERS)\n","\n","MAX_SEQUENCE = 25\n","\n","\n","def load_data(path):\n","    # 판다스를 통해서 데이터를 불러온다.\n","    data_df = pd.read_csv(path, header=0)\n","    # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n","    question, answer = list(data_df['Q']), list(data_df['A'])\n","\n","    return question, answer\n","\n","\n","def data_tokenizer(data):\n","    # 토크나이징 해서 담을 배열 생성\n","    words = []\n","    for sentence in data:\n","        # FILTERS = \"([~.,!?\\\"':;)(])\"\n","        # 위 필터와 같은 값들을 정규화 표현식을\n","        # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n","        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n","        for word in sentence.split():\n","            words.append(word)\n","    # 토그나이징과 정규표현식을 통해 만들어진\n","    # 값들을 넘겨 준다.\n","    return [word for word in words if word]\n","\n","\n","def prepro_like_morphlized(data):\n","    morph_analyzer = Okt()\n","    result_data = list()\n","    for seq in tqdm(data):\n","        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n","        result_data.append(morphlized_seq)\n","\n","    return result_data\n","\n","\n","def load_vocabulary(path, vocab_path, tokenize_as_morph=False):\n","    # 사전을 담을 배열 준비한다.\n","    vocabulary_list = []\n","    # 사전을 구성한 후 파일로 저장 진행한다.\n","    # 그 파일의 존재 유무를 확인한다.\n","    if not os.path.exists(vocab_path):\n","        # 이미 생성된 사전 파일이 존재하지 않으므로\n","        # 데이터를 가지고 만들어야 한다.\n","        # 그래서 데이터가 존재 하면 사전을 만들기 위해서\n","        # 데이터 파일의 존재 유무를 확인한다.\n","        if (os.path.exists(path)):\n","            # 데이터가 존재하니 판단스를 통해서\n","            # 데이터를 불러오자\n","            data_df = pd.read_csv(path, encoding='utf-8')\n","            # 판다스의 데이터 프레임을 통해서\n","            # 질문과 답에 대한 열을 가져 온다.\n","            question, answer = list(data_df['Q']), list(data_df['A'])\n","            if tokenize_as_morph:  # 형태소에 따른 토크나이져 처리\n","                question = prepro_like_morphlized(question)\n","                answer = prepro_like_morphlized(answer)\n","            data = []\n","            # 질문과 답변을 extend을\n","            # 통해서 구조가 없는 배열로 만든다.\n","            data.extend(question)\n","            data.extend(answer)\n","            # 토큰나이져 처리 하는 부분이다.\n","            words = data_tokenizer(data)\n","            # 공통적인 단어에 대해서는 모두\n","            # 필요 없으므로 한개로 만들어 주기 위해서\n","            # set해주고 이것들을 리스트로 만들어 준다.\n","            words = list(set(words))\n","            # 데이터 없는 내용중에 MARKER를 사전에\n","            # 추가 하기 위해서 아래와 같이 처리 한다.\n","            # 아래는 MARKER 값이며 리스트의 첫번째 부터\n","            # 순서대로 넣기 위해서 인덱스 0에 추가한다.\n","            # PAD = \"<PADDING>\"\n","            # STD = \"<START>\"\n","            # END = \"<END>\"\n","            # UNK = \"<UNKNWON>\"\n","            words[:0] = MARKER\n","        # 사전을 리스트로 만들었으니 이 내용을\n","        # 사전 파일을 만들어 넣는다.\n","        with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:\n","            for word in words:\n","                vocabulary_file.write(word + '\\n')\n","\n","    # 사전 파일이 존재하면 여기에서\n","    # 그 파일을 불러서 배열에 넣어 준다.\n","    with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:\n","        for line in vocabulary_file:\n","            vocabulary_list.append(line.strip())\n","\n","    # 배열에 내용을 키와 값이 있는\n","    # 딕셔너리 구조로 만든다.\n","    char2idx, idx2char = make_vocabulary(vocabulary_list)\n","    # 두가지 형태의 키와 값이 있는 형태를 리턴한다.\n","    # (예) 단어: 인덱스 , 인덱스: 단어)\n","    return char2idx, idx2char, len(char2idx)\n","\n","\n","def make_vocabulary(vocabulary_list):\n","    # 리스트를 키가 단어이고 값이 인덱스인\n","    # 딕셔너리를 만든다.\n","    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n","    # 리스트를 키가 인덱스이고 값이 단어인\n","    # 딕셔너리를 만든다.\n","    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n","    # 두개의 딕셔너리를 넘겨 준다.\n","    return char2idx, idx2char\n","\n","\n","def enc_processing(value, dictionary, tokenize_as_morph=False):\n","    # 인덱스 값들을 가지고 있는\n","    # 배열이다.(누적된다.)\n","    sequences_input_index = []\n","    # 하나의 인코딩 되는 문장의\n","    # 길이를 가지고 있다.(누적된다.)\n","    sequences_length = []\n","    # 형태소 토크나이징 사용 유무\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","\n","    # 한줄씩 불어온다.\n","    for sequence in value:\n","        # FILTERS = \"([~.,!?\\\"':;)(])\"\n","        # 정규화를 사용하여 필터에 들어 있는\n","        # 값들을 \"\" 으로 치환 한다.\n","        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","        # 하나의 문장을 인코딩 할때\n","        # 가지고 있기 위한 배열이다.\n","        sequence_index = []\n","        # 문장을 스페이스 단위로\n","        # 자르고 있다.\n","        for word in sequence.split():\n","            # 잘려진 단어들이 딕셔너리에 존재 하는지 보고\n","            # 그 값을 가져와 sequence_index에 추가한다.\n","            if dictionary.get(word) is not None:\n","                sequence_index.extend([dictionary[word]])\n","            # 잘려진 단어가 딕셔너리에 존재 하지 않는\n","            # 경우 이므로 UNK(2)를 넣어 준다.\n","            else:\n","                sequence_index.extend([dictionary[UNK]])\n","        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n","        if len(sequence_index) > MAX_SEQUENCE:\n","            sequence_index = sequence_index[:MAX_SEQUENCE]\n","        # 하나의 문장에 길이를 넣어주고 있다.\n","        sequences_length.append(len(sequence_index))\n","        # max_sequence_length보다 문장 길이가\n","        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n","        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n","        # 인덱스화 되어 있는 값을\n","        # sequences_input_index에 넣어 준다.\n","        sequences_input_index.append(sequence_index)\n","    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n","    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n","    # 사전 작업이다.\n","    # 넘파이 배열에 인덱스화된 배열과\n","    # 그 길이를 넘겨준다.\n","    return np.asarray(sequences_input_index), sequences_length\n","\n","\n","def dec_output_processing(value, dictionary, tokenize_as_morph=False):\n","    # 인덱스 값들을 가지고 있는\n","    # 배열이다.(누적된다)\n","    sequences_output_index = []\n","    # 하나의 디코딩 입력 되는 문장의\n","    # 길이를 가지고 있다.(누적된다)\n","    sequences_length = []\n","    # 형태소 토크나이징 사용 유무\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","    # 한줄씩 불어온다.\n","    for sequence in value:\n","        # FILTERS = \"([~.,!?\\\"':;)(])\"\n","        # 정규화를 사용하여 필터에 들어 있는\n","        # 값들을 \"\" 으로 치환 한다.\n","        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","        # 하나의 문장을 디코딩 할때 가지고\n","        # 있기 위한 배열이다.\n","        sequence_index = []\n","        # 디코딩 입력의 처음에는 START가 와야 하므로\n","        # 그 값을 넣어 주고 시작한다.\n","        # 문장에서 스페이스 단위별로 단어를 가져와서 딕셔너리의\n","        # 값인 인덱스를 넣어 준다.\n","        sequence_index = [dictionary[STD]] + [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n","        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n","        if len(sequence_index) > MAX_SEQUENCE:\n","            sequence_index = sequence_index[:MAX_SEQUENCE]\n","        # 하나의 문장에 길이를 넣어주고 있다.\n","        sequences_length.append(len(sequence_index))\n","        # max_sequence_length보다 문장 길이가\n","        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n","        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n","        # 인덱스화 되어 있는 값을\n","        # sequences_output_index 넣어 준다.\n","        sequences_output_index.append(sequence_index)\n","    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n","    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n","    # 사전 작업이다.\n","    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n","    return np.asarray(sequences_output_index), sequences_length\n","\n","\n","def dec_target_processing(value, dictionary, tokenize_as_morph=False):\n","    # 인덱스 값들을 가지고 있는\n","    # 배열이다.(누적된다)\n","    sequences_target_index = []\n","    # 형태소 토크나이징 사용 유무\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","    # 한줄씩 불어온다.\n","    for sequence in value:\n","        # FILTERS = \"([~.,!?\\\"':;)(])\"\n","        # 정규화를 사용하여 필터에 들어 있는\n","        # 값들을 \"\" 으로 치환 한다.\n","        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","        # 문장에서 스페이스 단위별로 단어를 가져와서\n","        # 딕셔너리의 값인 인덱스를 넣어 준다.\n","        # 디코딩 출력의 마지막에 END를 넣어 준다.\n","        sequence_index = [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n","        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n","        # 그리고 END 토큰을 넣어 준다\n","        if len(sequence_index) >= MAX_SEQUENCE:\n","            sequence_index = sequence_index[:MAX_SEQUENCE - 1] + [dictionary[END]]\n","        else:\n","            sequence_index += [dictionary[END]]\n","        # max_sequence_length보다 문장 길이가\n","        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n","        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n","        # 인덱스화 되어 있는 값을\n","        # sequences_target_index에 넣어 준다.\n","        sequences_target_index.append(sequence_index)\n","    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n","    # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n","    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n","    return np.asarray(sequences_target_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NBWOP3cHcBqF"},"outputs":[],"source":["PATH = 'ChatBotData_short.csv'\n","VOCAB_PATH = 'vocabulary.txt'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1XaWUt6cBqF"},"outputs":[],"source":["inputs, outputs = load_data(PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-BsNbPhcBqF"},"outputs":[],"source":["char2idx, idx2char, vocab_size = load_vocabulary(PATH, VOCAB_PATH, tokenize_as_morph=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"drfz8ntkcBqF"},"outputs":[],"source":["index_inputs, input_seq_len = enc_processing(inputs, char2idx, tokenize_as_morph=False)\n","index_outputs, output_seq_len = dec_output_processing(outputs, char2idx, tokenize_as_morph=False)\n","index_targets = dec_target_processing(outputs, char2idx, tokenize_as_morph=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2rm8Y5BcBqF"},"outputs":[],"source":["data_configs = {}\n","data_configs['char2idx'] = char2idx\n","data_configs['idx2char'] = idx2char\n","data_configs['vocab_size'] = vocab_size\n","data_configs['pad_symbol'] = PAD\n","data_configs['std_symbol'] = STD\n","data_configs['end_symbol'] = END\n","data_configs['unk_symbol'] = UNK"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZ45o0HacBqF"},"outputs":[],"source":["DATA_IN_PATH = './'\n","TRAIN_INPUTS = 'train_inputs.npy'\n","TRAIN_OUTPUTS = 'train_outputs.npy'\n","TRAIN_TARGETS = 'train_targets.npy'\n","DATA_CONFIGS = 'data_configs.json'\n","\n","np.save(open(DATA_IN_PATH + TRAIN_INPUTS, 'wb'), index_inputs)\n","np.save(open(DATA_IN_PATH + TRAIN_OUTPUTS , 'wb'), index_outputs)\n","np.save(open(DATA_IN_PATH + TRAIN_TARGETS , 'wb'), index_targets)\n","\n","json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u6gegD33cBqG","executionInfo":{"status":"ok","timestamp":1683501730442,"user_tz":-540,"elapsed":4,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}},"outputId":"60abcfe7-7ded-474f-ea7e-c1f6b880e8ae"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'<PAD>': 0,\n"," '<SOS>': 1,\n"," '<END>': 2,\n"," '<UNK>': 3,\n"," '구하셨나요': 4,\n"," '궁금해': 5,\n"," '뭘': 6,\n"," '갔어': 7,\n"," '설득해보세요': 8,\n"," '보인다': 9,\n"," '오늘': 10,\n"," '승진': 11,\n"," '좋을까': 12,\n"," '나라를': 13,\n"," '가끔은': 14,\n"," '있어도': 15,\n"," '감기': 16,\n"," '빨리': 17,\n"," '줄까': 18,\n"," '하세요': 19,\n"," '그': 20,\n"," '거짓말': 21,\n"," '돈은': 22,\n"," '나왔다': 23,\n"," '필요했던': 24,\n"," '설움': 25,\n"," '걸리겠어': 26,\n"," '안': 27,\n"," '가상화폐': 28,\n"," '가난한': 29,\n"," '바빠': 30,\n"," '켜놓고': 31,\n"," '잘생겼어': 32,\n"," '많이': 33,\n"," '질린다': 34,\n"," '뭐하는지': 35,\n"," '함께': 36,\n"," '달에는': 37,\n"," '필요한': 38,\n"," '같아': 39,\n"," '해': 40,\n"," '혼자인게': 41,\n"," '운동만': 42,\n"," '게': 43,\n"," '결단은': 44,\n"," '남자친구가': 45,\n"," '뭐가': 46,\n"," '끄고': 47,\n"," '싶어': 48,\n"," '쫄딱': 49,\n"," '들어올': 50,\n"," '잊고': 51,\n"," '가만': 52,\n"," '마음을': 53,\n"," '가스불': 54,\n"," '돌아가서': 55,\n"," '나': 56,\n"," '비싼데': 57,\n"," '좋아요': 58,\n"," '때까지': 59,\n"," '따라': 60,\n"," '집에': 61,\n"," '망함': 62,\n"," '혼자를': 63,\n"," '가스비': 64,\n"," '나갔어': 65,\n"," '좋을': 66,\n"," '땀난다': 67,\n"," '적당히': 68,\n"," '따뜻하게': 69,\n"," '믿어줘': 70,\n"," '좋다': 71,\n"," '데려가고': 72,\n"," '땀을': 73,\n"," '교회': 74,\n"," '운동을': 75,\n"," '해보세요': 76,\n"," '너무': 77,\n"," '절약해봐요': 78,\n"," '운동': 79,\n"," '선물로': 80,\n"," '어서': 81,\n"," '생일인데': 82,\n"," '다시': 83,\n"," '켜고': 84,\n"," '나오세요': 85,\n"," '전생에': 86,\n"," '집착하지': 87,\n"," '사람도': 88,\n"," '남자친구': 89,\n"," '자의': 90,\n"," '사세요': 91,\n"," '또': 92,\n"," '좀': 93,\n"," '빠를수록': 94,\n"," '다음': 95,\n"," '훈훈해': 96,\n"," '것': 97,\n"," '마세요': 98,\n"," '생각해보세요': 99,\n"," '가끔': 100,\n"," '새출발': 101,\n"," '즐기세요': 102,\n"," '그럴': 103,\n"," '평소에': 104,\n"," '나온거': 105,\n"," '식혀주세요': 106,\n"," '열': 107,\n"," '더': 108,\n"," '거예요': 109,\n"," '같아요': 110}"]},"metadata":{},"execution_count":10}],"source":["char2idx"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3woPEzSCcBqG","executionInfo":{"status":"ok","timestamp":1683501731656,"user_tz":-540,"elapsed":2,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}},"outputId":"78eaf29d-a875-4686-93c6-c0e2a46f5d93"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: '<PAD>',\n"," 1: '<SOS>',\n"," 2: '<END>',\n"," 3: '<UNK>',\n"," 4: '구하셨나요',\n"," 5: '궁금해',\n"," 6: '뭘',\n"," 7: '갔어',\n"," 8: '설득해보세요',\n"," 9: '보인다',\n"," 10: '오늘',\n"," 11: '승진',\n"," 12: '좋을까',\n"," 13: '나라를',\n"," 14: '가끔은',\n"," 15: '있어도',\n"," 16: '감기',\n"," 17: '빨리',\n"," 18: '줄까',\n"," 19: '하세요',\n"," 20: '그',\n"," 21: '거짓말',\n"," 22: '돈은',\n"," 23: '나왔다',\n"," 24: '필요했던',\n"," 25: '설움',\n"," 26: '걸리겠어',\n"," 27: '안',\n"," 28: '가상화폐',\n"," 29: '가난한',\n"," 30: '바빠',\n"," 31: '켜놓고',\n"," 32: '잘생겼어',\n"," 33: '많이',\n"," 34: '질린다',\n"," 35: '뭐하는지',\n"," 36: '함께',\n"," 37: '달에는',\n"," 38: '필요한',\n"," 39: '같아',\n"," 40: '해',\n"," 41: '혼자인게',\n"," 42: '운동만',\n"," 43: '게',\n"," 44: '결단은',\n"," 45: '남자친구가',\n"," 46: '뭐가',\n"," 47: '끄고',\n"," 48: '싶어',\n"," 49: '쫄딱',\n"," 50: '들어올',\n"," 51: '잊고',\n"," 52: '가만',\n"," 53: '마음을',\n"," 54: '가스불',\n"," 55: '돌아가서',\n"," 56: '나',\n"," 57: '비싼데',\n"," 58: '좋아요',\n"," 59: '때까지',\n"," 60: '따라',\n"," 61: '집에',\n"," 62: '망함',\n"," 63: '혼자를',\n"," 64: '가스비',\n"," 65: '나갔어',\n"," 66: '좋을',\n"," 67: '땀난다',\n"," 68: '적당히',\n"," 69: '따뜻하게',\n"," 70: '믿어줘',\n"," 71: '좋다',\n"," 72: '데려가고',\n"," 73: '땀을',\n"," 74: '교회',\n"," 75: '운동을',\n"," 76: '해보세요',\n"," 77: '너무',\n"," 78: '절약해봐요',\n"," 79: '운동',\n"," 80: '선물로',\n"," 81: '어서',\n"," 82: '생일인데',\n"," 83: '다시',\n"," 84: '켜고',\n"," 85: '나오세요',\n"," 86: '전생에',\n"," 87: '집착하지',\n"," 88: '사람도',\n"," 89: '남자친구',\n"," 90: '자의',\n"," 91: '사세요',\n"," 92: '또',\n"," 93: '좀',\n"," 94: '빠를수록',\n"," 95: '다음',\n"," 96: '훈훈해',\n"," 97: '것',\n"," 98: '마세요',\n"," 99: '생각해보세요',\n"," 100: '가끔',\n"," 101: '새출발',\n"," 102: '즐기세요',\n"," 103: '그럴',\n"," 104: '평소에',\n"," 105: '나온거',\n"," 106: '식혀주세요',\n"," 107: '열',\n"," 108: '더',\n"," 109: '거예요',\n"," 110: '같아요'}"]},"metadata":{},"execution_count":11}],"source":["idx2char"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hiO9fqyRcBqG"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvyckPcOcBqG"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}