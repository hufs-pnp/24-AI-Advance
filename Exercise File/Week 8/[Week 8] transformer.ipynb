{"cells":[{"cell_type":"markdown","metadata":{"id":"TXSgl_cSZWI8"},"source":["# 모듈 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19477,"status":"ok","timestamp":1705402777369,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"nJCye9i4Z4Dc","outputId":"236549a1-7f5f-4cbb-dbc6-e95f21a30a53"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting preprocess\n","  Downloading preprocess-2.0.0-py3-none-any.whl (12 kB)\n","Collecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from preprocess) (0.18.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n","Installing collected packages: preprocess, JPype1, konlpy\n","Successfully installed JPype1-1.5.0 konlpy-0.6.0 preprocess-2.0.0\n"]}],"source":["!pip install konlpy preprocess"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AV-FvWgQZWI9"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","\n","from konlpy.tag import Twitter\n","import pandas as pd\n","import tensorflow as tf\n","import enum\n","import os\n","import re\n","import json\n","from sklearn.model_selection import train_test_split\n","\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import matplotlib.pyplot as plt\n","\n","from preprocess import *"]},{"cell_type":"markdown","metadata":{"id":"0XO9m6U-ZWI-"},"source":["# 시각화 함수"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m8evqZv8ZWI-"},"outputs":[],"source":["def plot_graphs(history, string):\n","    plt.plot(history.history[string])\n","    plt.plot(history.history['val_'+string], '')\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(string)\n","    plt.legend([string, 'val_'+string])\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"UqbwnZiYZWI-"},"source":["# 학습 데이터 경로 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WWCNR1e4ZWI-"},"outputs":[],"source":["DATA_IN_PATH = '/content/'\n","DATA_OUT_PATH = '/content/'\n","TRAIN_INPUTS = 'train_inputs.npy'\n","TRAIN_OUTPUTS = 'train_outputs.npy'\n","TRAIN_TARGETS = 'train_targets.npy'\n","DATA_CONFIGS = 'data_configs.json'"]},{"cell_type":"markdown","metadata":{"id":"MjCalv0_ZWI-"},"source":["# 랜덤 시드 고정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQi7CygwZWI-"},"outputs":[],"source":["SEED_NUM = 1234\n","tf.random.set_seed(SEED_NUM)"]},{"cell_type":"markdown","metadata":{"id":"Jru07IHzZWI-"},"source":["# 파일 로드"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"997yZTfoZWI-","executionInfo":{"status":"error","timestamp":1705402784751,"user_tz":-540,"elapsed":468,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}},"outputId":"3cba5f1d-eb81-4372-9d92-a0f3aaba4c21","colab":{"base_uri":"https://localhost:8080/","height":233}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/train_inputs.npy'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-84ec00a70690>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_IN_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTRAIN_INPUTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mindex_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_IN_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTRAIN_OUTPUTS\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mindex_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_IN_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTRAIN_TARGETS\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprepro_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_IN_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mDATA_CONFIGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train_inputs.npy'"]}],"source":["index_inputs = np.load(open(DATA_IN_PATH + TRAIN_INPUTS, 'rb'))\n","index_outputs = np.load(open(DATA_IN_PATH + TRAIN_OUTPUTS , 'rb'))\n","index_targets = np.load(open(DATA_IN_PATH + TRAIN_TARGETS , 'rb'))\n","prepro_configs = json.load(open(DATA_IN_PATH + DATA_CONFIGS, 'r'))"]},{"cell_type":"markdown","metadata":{"id":"XcteVkO4ZWI-"},"source":["# 모델 하이퍼파라메터 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1MSFZpYYZWI-"},"outputs":[],"source":["char2idx = prepro_configs['char2idx']\n","end_index = prepro_configs['end_symbol']\n","model_name = 'transformer'\n","vocab_size = prepro_configs['vocab_size']\n","BATCH_SIZE = 2\n","MAX_SEQUENCE = 25\n","EPOCHS = 30\n","VALID_SPLIT = 0.1\n","\n","kargs = {'model_name': model_name,\n","         'num_layers': 2,\n","         'd_model': 512,\n","         'num_heads': 8,\n","         'dff': 2048,\n","         'input_vocab_size': vocab_size,\n","         'target_vocab_size': vocab_size,\n","         'maximum_position_encoding': MAX_SEQUENCE,\n","         'end_token_idx': char2idx[end_index],\n","         'rate': 0.1\n","        }"]},{"cell_type":"markdown","metadata":{"id":"mD27pcHiZWI_"},"source":["# 모델 선언 및 컴파일"]},{"cell_type":"markdown","metadata":{"id":"KFsPWvr2ZWI_"},"source":["## 패딩 및 포워드 마스킹"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRUx6Ym_ZWI_"},"outputs":[],"source":["def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","\n","    # add extra dimensions to add the padding\n","    # to the attention logits.\n","    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFiIASeDZWI_"},"outputs":[],"source":["def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask  # (seq_len, seq_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffRIUXCZZWI_"},"outputs":[],"source":["def create_masks(inp, tar):\n","    # Encoder padding mask\n","    enc_padding_mask = create_padding_mask(inp)\n","\n","    # Used in the 2nd attention block in the decoder.\n","    # This padding mask is used to mask the encoder outputs.\n","    dec_padding_mask = create_padding_mask(inp)\n","\n","    # Used in the 1st attention block in the decoder.\n","    # It is used to pad and mask future tokens in the input received by\n","    # the decoder.\n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","\n","    return enc_padding_mask, combined_mask, dec_padding_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_5iQE2xZWI_"},"outputs":[],"source":["enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(index_inputs, index_outputs)"]},{"cell_type":"markdown","metadata":{"id":"PzS7j3hVZWI_"},"source":["## 포지셔널 인코딩"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n5VjDvPPZWI_"},"outputs":[],"source":["def get_angles(pos, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * i//2) / np.float32(d_model))\n","    return pos * angle_rates"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VkgIxaaEZWI_"},"outputs":[],"source":["def positional_encoding(position, d_model):\n","    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model)\n","\n","    # apply sin to even indices in the array; 2i\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","    # apply cos to odd indices in the array; 2i+1\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ep07BljJZWI_"},"outputs":[],"source":["pos_encoding = positional_encoding(50, 512)\n","print (pos_encoding.shape)\n","\n","plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n","plt.xlabel('Depth')\n","plt.xlim((0, 512))\n","plt.ylabel('Position')\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"As2YOkrpZWJA"},"source":["## 어텐션"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABJhxHPqZWJA"},"outputs":[],"source":["def scaled_dot_product_attention(q, k, v, mask):\n","    \"\"\"Calculate the attention weights.\n","    q, k, v must have matching leading dimensions.\n","    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","    The mask has different shapes depending on its type(padding or look ahead)\n","    but it must be broadcastable for addition.\n","\n","    Args:\n","    q: query shape == (..., seq_len_q, depth)\n","    k: key shape == (..., seq_len_k, depth)\n","    v: value shape == (..., seq_len_v, depth_v)\n","    mask: Float tensor with shape broadcastable\n","          to (..., seq_len_q, seq_len_k). Defaults to None.\n","\n","    Returns:\n","    output, attention_weights\n","    \"\"\"\n","\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","\n","    # scale matmul_qk\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    # add the mask to the scaled tensor.\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)\n","\n","    # softmax is normalized on the last axis (seq_len_k) so that the scores\n","    # add up to 1.\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","    return output, attention_weights"]},{"cell_type":"markdown","metadata":{"id":"lH4eSFg6ZWJA"},"source":["## 멀티헤드 어텐션"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stO8FNWjZWJA"},"outputs":[],"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, **kargs):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = kargs['num_heads']\n","        self.d_model = kargs['d_model']\n","\n","        assert self.d_model % self.num_heads == 0\n","\n","        self.depth = self.d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(kargs['d_model'])\n","        self.wk = tf.keras.layers.Dense(kargs['d_model'])\n","        self.wv = tf.keras.layers.Dense(kargs['d_model'])\n","\n","        self.dense = tf.keras.layers.Dense(kargs['d_model'])\n","\n","    def split_heads(self, x, batch_size):\n","        \"\"\"Split the last dimension into (num_heads, depth).\n","        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","        \"\"\"\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)  # (batch_size, seq_len, d_model)\n","        k = self.wk(k)  # (batch_size, seq_len, d_model)\n","        v = self.wv(v)  # (batch_size, seq_len, d_model)\n","\n","        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","\n","        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","        scaled_attention, attention_weights = scaled_dot_product_attention(\n","            q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","        concat_attention = tf.reshape(scaled_attention,\n","                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","\n","        return output, attention_weights"]},{"cell_type":"markdown","metadata":{"id":"0DnrE-trZWJA"},"source":["## 포인트 와이즈 피드포워드 네트워크"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4liXE0IwZWJA"},"outputs":[],"source":["def point_wise_feed_forward_network(**kargs):\n","    return tf.keras.Sequential([\n","      tf.keras.layers.Dense(kargs['dff'], activation='relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(kargs['d_model'])  # (batch_size, seq_len, d_model)\n","    ])\n"]},{"cell_type":"markdown","metadata":{"id":"sDRs3baoZWJA"},"source":["## 인코더 레이어"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0H35AS-4ZWJA"},"outputs":[],"source":["class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, **kargs):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(**kargs)\n","        self.ffn = point_wise_feed_forward_network(**kargs)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n","        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n","\n","    def call(self, x, mask):\n","        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n","        attn_output = self.dropout1(attn_output)\n","        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n","\n","        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","        ffn_output = self.dropout2(ffn_output)\n","        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","\n","        return out2, attn_output"]},{"cell_type":"markdown","metadata":{"id":"U2ftyB31ZWJA"},"source":["## 디코더 레이어"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8V3y8UH9ZWJA"},"outputs":[],"source":["class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, **kargs):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.mha1 = MultiHeadAttention(**kargs)\n","        self.mha2 = MultiHeadAttention(**kargs)\n","\n","        self.ffn = point_wise_feed_forward_network(**kargs)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n","        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n","        self.dropout3 = tf.keras.layers.Dropout(kargs['rate'])\n","\n","\n","    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n","        # enc_output.shape == (batch_size, input_seq_len, d_model)\n","        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n","        attn1 = self.dropout1(attn1)\n","        out1 = self.layernorm1(attn1 + x)\n","\n","        attn2, attn_weights_block2 = self.mha2(\n","            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n","        attn2 = self.dropout2(attn2)\n","        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n","\n","        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n","        ffn_output = self.dropout3(ffn_output)\n","        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n","\n","        return out3, attn_weights_block1, attn_weights_block2"]},{"cell_type":"markdown","metadata":{"id":"IDLpHnrPZWJA"},"source":["## 인코더"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pggdtrTTZWJA"},"outputs":[],"source":["class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, **kargs):\n","        super(Encoder, self).__init__()\n","\n","        self.d_model = kargs['d_model']\n","        self.num_layers = kargs['num_layers']\n","\n","        self.embedding = tf.keras.layers.Embedding(kargs['input_vocab_size'], self.d_model)\n","        self.pos_encoding = positional_encoding(kargs['maximum_position_encoding'],\n","                                                self.d_model)\n","\n","\n","        self.enc_layers = [EncoderLayer(**kargs)\n","                           for _ in range(self.num_layers)]\n","\n","        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n","\n","    def call(self, x, mask):\n","        attn = None\n","        seq_len = tf.shape(x)[1]\n","\n","        # adding embedding and position encoding.\n","        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x)\n","\n","        for i in range(self.num_layers):\n","            x, attn = self.enc_layers[i](x, mask)\n","\n","        return x, attn  # (batch_size, input_seq_len, d_model)"]},{"cell_type":"markdown","metadata":{"id":"Uq5dKPjRZWJB"},"source":["## 디코더"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8VCazl2ZWJB"},"outputs":[],"source":["class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, **kargs):\n","        super(Decoder, self).__init__()\n","\n","        self.d_model = kargs['d_model']\n","        self.num_layers = kargs['num_layers']\n","\n","        self.embedding = tf.keras.layers.Embedding(kargs['target_vocab_size'], self.d_model)\n","        self.pos_encoding = positional_encoding(kargs['maximum_position_encoding'], self.d_model)\n","\n","        self.dec_layers = [DecoderLayer(**kargs)\n","                           for _ in range(self.num_layers)]\n","        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n","\n","    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n","        seq_len = tf.shape(x)[1]\n","        attention_weights = {}\n","\n","        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x)\n","\n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n","\n","            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","\n","        # x.shape == (batch_size, target_seq_len, d_model)\n","        return x, attention_weights"]},{"cell_type":"markdown","metadata":{"id":"1oj1qUTVZWJB"},"source":["## 트렌스포머 모델"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"in6TKTeMZWJB"},"outputs":[],"source":["class Transformer(tf.keras.Model):\n","    def __init__(self, **kargs):\n","        super(Transformer, self).__init__(name=kargs['model_name'])\n","        self.end_token_idx = kargs['end_token_idx']\n","\n","        self.encoder = Encoder(**kargs)\n","        self.decoder = Decoder(**kargs)\n","\n","        self.final_layer = tf.keras.layers.Dense(kargs['target_vocab_size'])\n","\n","    def call(self, x):\n","        inp, tar = x\n","\n","        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n","        enc_output, attn = self.encoder(inp, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n","\n","        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n","        dec_output, attn = self.decoder(\n","            tar, enc_output, look_ahead_mask, dec_padding_mask)\n","\n","        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n","\n","        return final_output #, attn\n","\n","    def inference(self, x):\n","        inp = x\n","        tar = tf.expand_dims([STD_INDEX], 0)\n","\n","        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n","        enc_output = self.encoder(inp, enc_padding_mask)\n","\n","        predict_tokens = list()\n","        for t in range(0, MAX_SEQUENCE):\n","            dec_output, _ = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n","            final_output = self.final_layer(dec_output)\n","            outputs = tf.argmax(final_output, -1).numpy()\n","            pred_token = outputs[0][-1]\n","            if pred_token == self.end_token_idx:\n","                break\n","            predict_tokens.append(pred_token)\n","            tar = tf.expand_dims([STD_INDEX] + predict_tokens, 0)\n","            _, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n","\n","        return predict_tokens"]},{"cell_type":"markdown","metadata":{"id":"25Ra5MueZWJB"},"source":["## 모델 로스 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yx6GkkzJZWJB"},"outputs":[],"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n","\n","def loss(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)\n","\n","def accuracy(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n","    pred *= mask\n","    acc = train_accuracy(real, pred)\n","\n","    return tf.reduce_mean(acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_abxNtXwZWJB"},"outputs":[],"source":["model = Transformer(**kargs)\n","model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n","              loss=loss,\n","              metrics=[accuracy])"]},{"cell_type":"markdown","metadata":{"id":"8wE51PZOZWJB"},"source":["# Callback 선언"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_p0kZqRZZWJB","scrolled":false},"outputs":[],"source":["# overfitting을 막기 위한 ealrystop 추가\n","earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10)\n","# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n","# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\n","\n","checkpoint_path = DATA_OUT_PATH + model_name + '/weights.h5'\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","# Create path if exists\n","if os.path.exists(checkpoint_dir):\n","    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n","else:\n","    os.makedirs(checkpoint_dir, exist_ok=True)\n","    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n","\n","\n","cp_callback = ModelCheckpoint(\n","    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"]},{"cell_type":"markdown","metadata":{"id":"AOblTqf5ZWJB"},"source":["# 모델 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9706tiVSZWJB"},"outputs":[],"source":["history = model.fit([index_inputs, index_outputs], index_targets,\n","                    batch_size=BATCH_SIZE, epochs=EPOCHS,\n","                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"]},{"cell_type":"markdown","metadata":{"id":"B71JzuDyZWJB"},"source":["# 결과 플롯"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h16igFzAZWJB"},"outputs":[],"source":["plot_graphs(history, 'accuracy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SgVaFXeIZWJC"},"outputs":[],"source":["plot_graphs(history, 'loss')"]},{"cell_type":"markdown","metadata":{"id":"p6hncNLJZWJC"},"source":["# 베스트 모델 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CjtnWULyZWJC"},"outputs":[],"source":["SAVE_FILE_NM = 'weights.h5'\n","\n","model.load_weights(os.path.join(DATA_OUT_PATH, model_name, SAVE_FILE_NM))"]},{"cell_type":"markdown","metadata":{"id":"zMI7AV_CZWJC"},"source":["# 모델 결과 출력하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Po88hl9lZWJC"},"outputs":[],"source":["char2idx = prepro_configs['char2idx']\n","idx2char = prepro_configs['idx2char']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubwBzYMMZWJC"},"outputs":[],"source":["text = \"남자친구 승진 선물로 뭐가 좋을까?\"\n","test_index_inputs, _ = enc_processing([text], char2idx)\n","outputs = model.inference(test_index_inputs)\n","\n","print(' '.join([idx2char[str(o)] for o in outputs]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vX8Xu3FvZWJC"},"outputs":[],"source":["text = \"아 배고프다.\"\n","test_index_inputs, _ = enc_processing([text], char2idx)\n","outputs = model.inference(test_index_inputs)\n","\n","print(' '.join([idx2char[str(o)] for o in outputs]))"]},{"cell_type":"code","source":[],"metadata":{"id":"n1t-C_yRJWVs"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}